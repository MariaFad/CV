{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdae154f-355e-49a5-b617-f7bc346beaec",
   "metadata": {},
   "source": [
    "# Marketing Mix Modeling\n",
    "## Marketing Mix Modeling - a method that helps to quantify the impact of marketing investments on some target-variable.  Unlike the usual regression, which can only analyze dependencies between two variables, MMM focuses on how several variables affect the result.\n",
    "\n",
    "## The main advantage of Marketing Mix Modeling is that the model takes into account two important advertising effects:\n",
    "### saturation - cannot achieve multiple increase of variable target at huge cost\n",
    "### ad-stock - when money is spent on advertising today, but tomorrow (and even after the end of the campaign) the effect remains in people’s memories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca70d4-7fcf-4162-88c6-4456c3c28719",
   "metadata": {},
   "source": [
    "## Google’s LightweightMMM library was used for the Marketing Mix Modeling method. This library is designed as a faster and easier solution that allows to get detailed results for analysis. In addition to the standard approach, LightweightMMM offers a hierarchical approach. If there is data at the state or regional level, this hierarchical approach based on geography can give more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21afc03-6499-4927-ba3d-1533837b130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sources:\n",
    "# https://colab.research.google.com/drive/1JjFFXVsni3KQ73bSjiutZ9tLjQlcI652?pli=1#scrollTo=qtt9L4aPL-cs\n",
    "# https://towardsdatascience.com/media-mix-modeling-how-to-measure-the-effectiveness-of-advertising-with-python-lightweightmmm-b6d7de110ae6/\n",
    "# https://medium.com/@mail2rajivgopinath/setting-priors-in-bayesian-marketing-mix-modeling-mmm-9f1b7469d96c\n",
    "# https://medium.com/@thomascgeorgiou/media-mix-modelling-measuring-advertising-effectiveness-using-googles-lightweightmmm-python-80cb4974b6ce\n",
    "# https://medium.com/@camilojaure/exploring-the-feasibility-of-lightweight-mmm-for-holistic-marketing-attribution-d587bcff0429\n",
    "# https://towardsdatascience.com/mastering-marketing-mix-modelling-in-python-7bbfe31360f9/\n",
    "# https://www.kaggle.com/code/jordangronkowski/lightweightmmm-for-marketing-mix-modeling\n",
    "# https://github.com/google/lightweight_mmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7db3ed-45e5-4859-b492-ebd53b6823fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import sys\n",
    "import statsmodels.api as sm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sb\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3292a395-1e47-478e-9fd9-2da9696b2173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import jax.numpy and any other library we might need.\n",
    "import jax.numpy as jnp\n",
    "import numpyro\n",
    "\n",
    "# Import the relevant modules of the library\n",
    "from lightweight_mmm import lightweight_mmm\n",
    "from lightweight_mmm import optimize_media\n",
    "from lightweight_mmm import plot\n",
    "from lightweight_mmm import preprocessing\n",
    "from lightweight_mmm import utils\n",
    "from lightweight_mmm import models\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eeb10e-62f6-4b40-94c0-d0496faf6fd9",
   "metadata": {},
   "source": [
    "# Explarotary Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189078c-1095-4ca1-a265-e6e163c8a250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(f'Data_for_MMM.xlsx')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c813f14c-5846-48c0-b1c3-d79e8eef0c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Interpolation for macroeconomic factors\n",
    "df[\"macroeconomic_factor_1\"] = df[\"macroeconomic_factor_1_inter\"].interpolate()\n",
    "df[\"macroeconomic_factor_2\"] = df[\"macroeconomic_factor_2_inter\"].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa67d35-98d2-4dfc-ac40-b40dc84af008",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df323cff-95ef-4fe3-94ef-e302d93bdb87",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9fc5ae-9cbb-4a7f-a02c-b6136e2420a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = df.drop('sales', axis=1).corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ad311-9d7c-4b82-b47c-bf488a93cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting correlation heatmap\n",
    "dataplot = sb.heatmap(df.corr(numeric_only=True), cmap=\"YlGnBu\", annot=True)\n",
    "\n",
    "# Displaying heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b705f-123e-4882-86b4-d41e61740f94",
   "metadata": {},
   "source": [
    "## Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac755fc8-f66e-49ab-966c-8c12cae98a51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variables = ['media_channel_1', 'media_channel_2', 'media_channel_3', 'media_channel_4', 'media_channel_5', \n",
    "             'macroeconomic_factor_1', 'macroeconomic_factor_2', \n",
    "             'macroeconomic_factor_1_inter', 'macroeconomic_factor_2_inter']\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "X = df[variables]\n",
    "vif_data['feature'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2569d9-6186-41f3-a314-e9497033103b",
   "metadata": {},
   "source": [
    "## Plot to vizualize the amount of media channel expenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86395a64-73b2-4ec2-beb5-c43600b834e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Stacked Area Plot\n",
    "df[['media_channel_1', 'media_channel_2', 'media_channel_3', 'media_channel_4', 'media_channel_5']].plot.area(alpha=0.8, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985e731-2dda-49a2-a71d-868f98ea3bd7",
   "metadata": {},
   "source": [
    "# Quality Data Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6dc865-7118-4572-a111-2196e4ed0fd5",
   "metadata": {},
   "source": [
    "## Test, train data and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a9277-34e7-4b4e-aa7e-2068dccc3342",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdsp_cols = ['media_channel_1', 'media_channel_2', 'media_channel_3', 'media_channel_4', 'media_channel_5']\n",
    "test_data_period_size = 4\n",
    "costs_multiply = 0.15\n",
    "df['index1'] = df.index\n",
    "control_vars = ['macroeconomic_factor_1', 'macroeconomic_factor_2']\n",
    "sales_cols =['sales']\n",
    "\n",
    "df_main = df[['index1']+sales_cols+mdsp_cols+control_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103e415-08bc-4853-891a-926b22621b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 105\n",
    "data_size = len(df_main)\n",
    "\n",
    "n_media_channels = len(mdsp_cols)\n",
    "n_extra_features = len(control_vars)\n",
    "media_data = df_main[mdsp_cols].to_numpy()\n",
    "extra_features = df_main[control_vars].to_numpy()\n",
    "target = df_main['sales'].to_numpy()\n",
    "costs = df_main[mdsp_cols].sum().to_numpy()\n",
    "\n",
    "# Split and scale data.\n",
    "test_data_period_size = test_data_period_size\n",
    "split_point = data_size - test_data_period_size\n",
    "# Media data\n",
    "media_data_train = media_data[:split_point, ...]\n",
    "media_data_test = media_data[split_point:, ...]\n",
    "# Extra features\n",
    "extra_features_train = extra_features[:split_point, ...]\n",
    "extra_features_test = extra_features[split_point:, ...]\n",
    "# Target\n",
    "target_train = target[:split_point]\n",
    "target_test = target[split_point:]\n",
    "\n",
    "def nonzero_mean(arr):\n",
    "    return jnp.nanmean(jnp.where(arr != 0, arr, jnp.nan))\n",
    "\n",
    "media_scaler = preprocessing.CustomScaler(divide_operation=nonzero_mean)\n",
    "\n",
    "extra_features_scaler = preprocessing.CustomScaler(divide_operation=nonzero_mean)\n",
    "\n",
    "target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "# multiply_by = 0.15 for normalizing data\n",
    "if costs_multiply == 0:\n",
    "    cost_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "else:\n",
    "    cost_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean, multiply_by = costs_multiply)\n",
    "\n",
    "media_data_train = media_scaler.fit_transform(media_data_train)\n",
    "extra_features_train = extra_features_scaler.fit_transform(extra_features_train)\n",
    "target_train = target_scaler.fit_transform(target_train)\n",
    "costs = cost_scaler.fit_transform(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7aea3-45de-418d-8853-bdcbba2d94de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlations, variances, spend_fractions, variance_inflation_factors = preprocessing.check_data_quality(\n",
    "    media_data=media_scaler.transform(media_data),\n",
    "    target_data=target_scaler.transform(target),\n",
    "    cost_data=costs,\n",
    "    extra_features_data=extra_features_scaler.transform(extra_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868c0f1f-8224-487e-83af-2601a5f77267",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e1edc-74d9-4802-b236-dc54e2005db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below cell shows the correlation matrix between all the features, and between each feature and the target. \n",
    "# Very positive or very negative correlations (with an absolute value above, say, 0.7 or so) \n",
    "# should be treated with caution. In this case you might consider dropping or \n",
    "# merging highly correlated features.\n",
    "correlations[0].style.background_gradient(cmap='RdBu', vmin=-1, vmax=1).format(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5032763-1400-4849-a594-56384de5e6a9",
   "metadata": {},
   "source": [
    "## Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030aa490-4c02-45d9-90d4-bfe613253763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below cell shows the variance of each feature over time. \n",
    "# Variances which are lower than the specified low_variance_threshold or higher \n",
    "# than the specified high_variance_threshold are marked in red. \n",
    "# Make sure you are passing the scaled versions of your media_data and extra_features_data \n",
    "# to the data quality checker for these variances before running this check!\n",
    "\n",
    "def highlight_variances(x: float, \n",
    "                        low_variance_threshold: float=1.0e-3, \n",
    "                        high_variance_threshold: float=3.0) -> str:\n",
    "\n",
    "    if x < low_variance_threshold or x > high_variance_threshold:\n",
    "      weight = 'bold'\n",
    "      color = 'red'\n",
    "    else:\n",
    "      weight = 'normal'\n",
    "      color = 'black'\n",
    "    style = f'font-weight: {weight}; color: {color}'\n",
    "    return style\n",
    "\n",
    "variances.style.format(precision=4).applymap(highlight_variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f20b228-4dc8-48b3-9250-82f264717dd7",
   "metadata": {},
   "source": [
    "## Low spend fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaea095-91bc-47d0-a377-325621284fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_low_spend_fractions(x: float,\n",
    "                                  low_spend_threshold: float=0.01) -> str:\n",
    "    if x < low_spend_threshold:\n",
    "      weight = 'bold'\n",
    "      color = 'red'\n",
    "    else:\n",
    "      weight = 'normal'\n",
    "      color = 'black'\n",
    "    style = f'font-weight: {weight}; color: {color}'\n",
    "    return style\n",
    "\n",
    "spend_fractions.style.format(precision=4).applymap(highlight_low_spend_fractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8856dbb-32e0-457d-99ff-3bc4949710b5",
   "metadata": {},
   "source": [
    "## Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f492ee9-8875-4299-a315-bb2806156ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While checking the correlation matrix in step 1 is usually sufficient \n",
    "# for detecting obvious multicollinearity in a dataset, the variance inflation factor is \n",
    "# technically the best metric for identifying multicollinear features. \n",
    "# Here we list the variance inflation factors for all features. If the number is too high \n",
    "# (we use a threshold here of 7, but feel free to adjust to your use case) \n",
    "# you might consider dropping or merging features with high variance inflation factors.\n",
    "\n",
    "def highlight_high_vif_values(x: float,\n",
    "                              high_vif_threshold: float=7.0) -> str:\n",
    "    if x > high_vif_threshold:\n",
    "      weight = 'bold'\n",
    "      color = 'red'\n",
    "    else:\n",
    "      weight = 'normal'\n",
    "      color = 'black'\n",
    "    style = f'font-weight: {weight}; color: {color}'\n",
    "    return style\n",
    "\n",
    "variance_inflation_factors.style.format(precision=4).applymap(highlight_high_vif_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4481fc-feda-4c7c-82fb-d2b925d3b4ab",
   "metadata": {},
   "source": [
    "# Marketing Mix Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36753b0a-ccb1-41f7-a59e-5248ea168734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEED = 105\n",
    "data_size = len(df_main)\n",
    "\n",
    "n_media_channels = len(mdsp_cols)\n",
    "n_extra_features = len(control_vars)\n",
    "media_data = df_main[mdsp_cols].to_numpy()\n",
    "extra_features = df_main[control_vars].to_numpy()\n",
    "target = df_main['sales'].to_numpy()\n",
    "costs = df_main[mdsp_cols].sum().to_numpy()\n",
    "\n",
    "# Split and scale data.\n",
    "test_data_period_size = test_data_period_size\n",
    "split_point = data_size - test_data_period_size\n",
    "# Media data\n",
    "media_data_train = media_data[:split_point, ...]\n",
    "media_data_test = media_data[split_point:, ...]\n",
    "# Extra features\n",
    "extra_features_train = extra_features[:split_point, ...]\n",
    "extra_features_test = extra_features[split_point:, ...]\n",
    "# Target\n",
    "target_train = target[:split_point]\n",
    "target_test = target[split_point:]\n",
    "\n",
    "def nonzero_mean(arr):\n",
    "    return jnp.nanmean(jnp.where(arr != 0, arr, jnp.nan))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "media_scaler = preprocessing.CustomScaler(divide_operation=nonzero_mean)\n",
    "\n",
    "#extra_features_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "#extra_features_scaler = StandardScaler()\n",
    "\n",
    "extra_features_scaler = preprocessing.CustomScaler(divide_operation=nonzero_mean)\n",
    "\n",
    "target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "# multiply_by = 0.15 for normalizing data\n",
    "if costs_multiply == 0:\n",
    "    cost_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "else:\n",
    "    cost_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean, multiply_by = costs_multiply)\n",
    "#cost_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "\n",
    "media_data_train = media_scaler.fit_transform(media_data_train)\n",
    "extra_features_train = extra_features_scaler.fit_transform(extra_features_train)\n",
    "target_train = target_scaler.fit_transform(target_train)\n",
    "costs = cost_scaler.fit_transform(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d87bf-f8d7-4b72-b5a4-2cb980f3e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate Weighted Average Percentage Error\n",
    "def wape(y_true, y_pred):\n",
    "    error = np.abs(y_true - y_pred)\n",
    "    return np.sum(error) / np.sum(y_true)\n",
    "\n",
    "# function to get data from marketing mix modeling summary\n",
    "def summary_df(s):\n",
    "    # Custom processing \n",
    "    # Due to noise in output I couldn't get reading directly from string buffer into pandas read csv to function)\n",
    "    # Row separators are '\\n' and Col separators are whitespace '\\\\s+'\n",
    "    rows = []\n",
    "    cols = re.split('\\\\s+', s.split('\\n')[1])[1:]\n",
    "    print(s.split('\\n')[-2:][0]) # Divergences\n",
    "    for row in [ln for i, ln in enumerate(s.split('\\n')) if i > 1]:\n",
    "        split_row = re.split('\\\\s+', row)[1:]\n",
    "        # For 3D parameters (gamma_seasonality)\n",
    "        if len(split_row) == 7:\n",
    "            rows += [['', *split_row]]\n",
    "        elif len(split_row) == 8:\n",
    "            rows += [split_row]\n",
    "\n",
    "    return pd.DataFrame(rows, columns=['Param', *cols] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05530e00-0729-4949-8ba4-1f5881ef3c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to choose the best model parameters on a test dataset\n",
    "# to identify model quality MAPE metric (also WAPE metric) is calculated and R_hat is checked to be less than 1.1\n",
    "\n",
    "def mmm_test_choose(df):\n",
    "    adstock_models = [\"adstock\", \"hill_adstock\", \"carryover\"]\n",
    "    degrees_season = [1,2,3]\n",
    "    \n",
    "    best_mape = float('inf')\n",
    "    best_model = None\n",
    "    for model_name in adstock_models:\n",
    "      for degrees in degrees_season:\n",
    "        mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n",
    "        mmm.fit(media=media_data_train,\n",
    "                media_prior=costs,\n",
    "                target=target_train,\n",
    "                extra_features=extra_features_train,\n",
    "                number_warmup=1000,\n",
    "                number_samples=1000,\n",
    "               # number_chains=1,\n",
    "                degrees_seasonality=degrees,\n",
    "               # weekday_seasonality=True,\n",
    "                seasonality_frequency=12,\n",
    "                # custom_priors = {'lag_weight': {'concentration1': 4., 'concentration0': 1.}},\n",
    "                seed=105)\n",
    "        prediction = mmm.predict(\n",
    "        media=media_scaler.transform(media_data_test),\n",
    "        extra_features=extra_features_scaler.transform(extra_features_test),\n",
    "        target_scaler=target_scaler)\n",
    "        p = prediction.mean(axis=0)\n",
    "        mape = mean_absolute_percentage_error(target_test, p)\n",
    "    \n",
    "        f = io.StringIO()\n",
    "        with redirect_stdout(f):\n",
    "            mmm.print_summary()\n",
    "        df_mmm = summary_df(f.getvalue())\n",
    "        if len(df_mmm.loc[df_mmm.r_hat.astype(float) >= 1.1]) > 0:\n",
    "            r_hat = \"R_HAT >= 1.1\"\n",
    "        else:\n",
    "            r_hat = \"R_HAT ok\"\n",
    "        print(f\"model_name={model_name} degrees={degrees} MAPE={mape} samples={p[:3]} {r_hat}\")\n",
    "    \n",
    "        print(f\"WAPE_test:{wape(target_test, p)*100} {r_hat}\\n\")\n",
    "          \n",
    "        if mape < best_mape and r_hat == \"R_HAT ok\":\n",
    "          best_mape = mape\n",
    "          best_model = f\"model_name={model_name} degrees={degress_seasonality} mape={mape}\"\n",
    "          \n",
    "    print(\"Best Performing Model:\", best_model, \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a0f7c6-b30c-4f1d-b323-6e04520dfde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes all needed data for mmm model (marketing mix model) and all needed technical parameters\n",
    "# as a result, a data frame with percentages of media contribution for each media channel towards the target value is provided for each month\n",
    "\n",
    "def function_mmm(mdsp_cols, control_vars, test_data_period_size, model_name, degrees_seasonality, costs_multiply):\n",
    "    df['index1'] = df.index\n",
    "    sales_cols =['sales']\n",
    "    \n",
    "    df_main = df[['index1']+sales_cols+mdsp_cols+control_vars]\n",
    "\n",
    "    SEED = 105\n",
    "    data_size = len(df_main)\n",
    "    \n",
    "    n_media_channels = len(mdsp_cols)\n",
    "    n_extra_features = len(control_vars)\n",
    "    media_data = df_main[mdsp_cols].to_numpy()\n",
    "    extra_features = df_main[control_vars].to_numpy()\n",
    "    target = df_main['sales'].to_numpy()\n",
    "    costs = df_main[mdsp_cols].sum().to_numpy()\n",
    "    \n",
    "    # Split and scale data.\n",
    "    # test_data_period_size = test_data_period_size\n",
    "    split_point = data_size - test_data_period_size\n",
    "    # Media data\n",
    "    media_data_train = media_data[:split_point, ...]\n",
    "    media_data_test = media_data[split_point:, ...]\n",
    "    # Extra features\n",
    "    extra_features_train = extra_features[:split_point, ...]\n",
    "    extra_features_test = extra_features[split_point:, ...]\n",
    "    # Target\n",
    "    target_train = target[:split_point]\n",
    "    target_test = target[split_point:]\n",
    "    \n",
    "    def nonzero_mean(arr):\n",
    "        return jnp.nanmean(jnp.where(arr != 0, arr, jnp.nan))\n",
    "    \n",
    "    media_scaler = preprocessing.CustomScaler(divide_operation=nonzero_mean)\n",
    "    extra_features_scaler = preprocessing.CustomScaler(divide_operation=nonzero_mean)\n",
    "    \n",
    "    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "    # multiply_by = 0.15 for normalizing data\n",
    "    if costs_multiply == 0:\n",
    "        cost_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "    else:\n",
    "        cost_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean, multiply_by = costs_multiply)\n",
    "    #cost_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "    \n",
    "    media_data_train = media_scaler.fit_transform(media_data_train)\n",
    "    extra_features_train = extra_features_scaler.fit_transform(extra_features_train)\n",
    "    target_train = target_scaler.fit_transform(target_train)\n",
    "    costs = cost_scaler.fit_transform(costs)\n",
    "\n",
    "    print(f\"costs_multiply: {costs_multiply}\\n\\ncontrol_vars = {control_vars}\\n\\nmdsp_cols = {mdsp_cols}\\n\\nmodel_name: {model_name}, deg_seasonality: {degrees_seasonality}\")\n",
    "    \n",
    "    number_warmup=1000\n",
    "    number_samples=1000\n",
    "    mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n",
    "    \n",
    "    mmm.fit(media=media_data_train, media_prior=costs, target=target_train,\n",
    "             extra_features=extra_features_train,\n",
    "            degrees_seasonality=degrees_seasonality,\n",
    "            seasonality_frequency=12,\n",
    "            number_warmup=number_warmup, number_samples=number_samples, media_names = mdsp_cols,\n",
    "            seed=SEED)\n",
    "\n",
    "    '''Once training is finished, you can check the summary of your trace: \n",
    "    The important point here is to check whether r hat values for all parameters are less than 1.1. \n",
    "    This is a checkpoint when you run Bayesian modeling.'''\n",
    "    # Capture Output to stdout by print summary\n",
    "    f = io.StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        mmm.print_summary()\n",
    "    df_mmm = summary_df(f.getvalue())\n",
    "    \n",
    "    if len(df_mmm.loc[df_mmm.r_hat.astype(float) >= 1.1]) > 0:\n",
    "        print(\"R_HAT >= 1.1\")\n",
    "    else:\n",
    "        print(\"R_HAT ok\")\n",
    "\n",
    "    plot.plot_model_fit(mmm, target_scaler=target_scaler).savefig(f'prediction_{added_dt}.png')\n",
    "\n",
    "    media_contribution, roi_hat = mmm.get_posterior_metrics(target_scaler=target_scaler, cost_scaler=cost_scaler)\n",
    "    plot.plot_media_baseline_contribution_area_plot(media_mix_model=mmm,\n",
    "                                                    target_scaler=target_scaler,\n",
    "                                                    fig_size=(20,10),\n",
    "                                                    channel_names = mdsp_cols\n",
    "                                                    ).savefig(f'media_contr_{added_dt}.png')\n",
    "\n",
    "    #WAPE\n",
    "    true_train = target_scaler.inverse_transform(target_train)\n",
    "    posterior_pred = mmm.trace[\"mu\"]\n",
    "    posterior_pred = target_scaler.inverse_transform(posterior_pred)\n",
    "    p = posterior_pred.mean(axis = 0)\n",
    "    \n",
    "    print(f\"WAPE_train:{wape(true_train, p)*100}\\n\")\n",
    "\n",
    "    \n",
    "    # Getting percentages of contribution\n",
    "    media_contributionss = jnp.einsum(\"stc, sc->stc\",\n",
    "                                  mmm.trace[\"media_transformed\"],\n",
    "                                  mmm.trace[\"coef_media\"])\n",
    "\n",
    "    mean_contributions = np.mean(media_contributionss, axis=0) # Shape: (51, 6)\n",
    "\n",
    "    total_contributions_per_time = np.sum(mean_contributions, axis=1)  # Shape: (51,)\n",
    "    \n",
    "    # Step 3: Calculate percentage contribution for each channel at each time point\n",
    "    percentage_contributions = (mean_contributions.T / total_contributions_per_time).T * 100  # Shape: (51, 6)\n",
    "    \n",
    "    # Step 4: Create a DataFrame for better visualization\n",
    "    df['month'] = df['index1']\n",
    "    time_points_range = df['index1']\n",
    "    channel_names = mdsp_cols\n",
    "    percentage_contributions_df = pd.DataFrame(percentage_contributions, columns=channel_names, index=time_points_range)\n",
    "    mean_contributions_df = pd.DataFrame(mean_contributions, columns=channel_names, index=time_points_range)\n",
    "    \n",
    "    # composing the final data frame\n",
    "    percentages = df[['yyyymm', 'index1']].merge(percentage_contributions_df.reset_index(), on = 'index1', how = 'outer')\n",
    "    percentages = percentages.drop(columns = 'index1', axis = 1)\n",
    "    \n",
    "    return percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a0cc8-1d74-47c2-a39c-b6a580dfae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the right parametrs for the model\n",
    "mmm_test_choose(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86394136-9e82-43f6-9062-9b3ab4e12ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the monthly data frame with percentages of media contribution for each media channel towards the target value\n",
    "df_mmm = mmm(i, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5297b968-7414-49ab-97f0-09927fdaa8c6",
   "metadata": {},
   "source": [
    "# Checking whether interpolated macroeconomic factors and adding the holidays data improve the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a6333-8745-4b31-9a0c-5a133452cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes all needed data for mmm model (marketing mix modeling) and all needed technical parameters\n",
    "# as a result, data for considering whether interpolation and holidays improve the model\n",
    "\n",
    "def function_mmm(mdsp_cols, control_vars, test_data_period_size,  model_name, degrees_seasonality, costs_multiply):\n",
    "    df['index1'] = df.index\n",
    "    sales_cols =['sales']\n",
    "    \n",
    "    df_main = df[['index1']+sales_cols+mdsp_cols \n",
    "    +control_vars\n",
    "    ]\n",
    "\n",
    "    SEED = 105\n",
    "    data_size = len(df_main)\n",
    "    \n",
    "    n_media_channels = len(mdsp_cols)\n",
    "    n_extra_features = len(control_vars)\n",
    "    media_data = df_main[mdsp_cols].to_numpy()\n",
    "    extra_features = df_main[control_vars].to_numpy()\n",
    "    target = df_main['sales'].to_numpy()\n",
    "    costs = df_main[mdsp_cols].sum().to_numpy()\n",
    "    \n",
    "    # Split and scale data.\n",
    "    # test_data_period_size = test_data_period_size\n",
    "    split_point = data_size - test_data_period_size\n",
    "    # Media data\n",
    "    media_data_train = media_data[:split_point, ...]\n",
    "    media_data_test = media_data[split_point:, ...]\n",
    "    # Extra features\n",
    "    extra_features_train = extra_features[:split_point, ...]\n",
    "    extra_features_test = extra_features[split_point:, ...]\n",
    "    # Target\n",
    "    target_train = target[:split_point]\n",
    "    target_test = target[split_point:]\n",
    "    \n",
    "    def nonzero_mean(arr):\n",
    "        return jnp.nanmean(jnp.where(arr != 0, arr, jnp.nan))\n",
    "    \n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    media_scaler = preprocessing.CustomScaler(divide_operation=nonzero_mean)\n",
    "    \n",
    "    #extra_features_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "    #extra_features_scaler = StandardScaler()\n",
    "    \n",
    "    extra_features_scaler = preprocessing.CustomScaler(divide_operation=nonzero_mean)\n",
    "    \n",
    "    target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "    # multiply_by = 0.15 for normalizing data\n",
    "    if costs_multiply == 0:\n",
    "        cost_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "    else:\n",
    "        cost_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean, multiply_by = costs_multiply)\n",
    "    #cost_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
    "    \n",
    "    media_data_train = media_scaler.fit_transform(media_data_train)\n",
    "    extra_features_train = extra_features_scaler.fit_transform(extra_features_train)\n",
    "    target_train = target_scaler.fit_transform(target_train)\n",
    "    costs = cost_scaler.fit_transform(costs)\n",
    "\n",
    "    number_warmup=1000\n",
    "    number_samples=1000\n",
    "    mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n",
    "    \n",
    "    mmm.fit(media=media_data_train, media_prior=costs, target=target_train,\n",
    "             extra_features=extra_features_train,\n",
    "            degrees_seasonality=degrees_seasonality,\n",
    "            seasonality_frequency=12,\n",
    "            number_warmup=number_warmup, number_samples=number_samples, media_names = mdsp_cols,\n",
    "            seed=SEED)\n",
    "\n",
    "    prediction = mmm.predict(\n",
    "    media=media_scaler.transform(media_data_test),\n",
    "    extra_features=extra_features_scaler.transform(extra_features_test),\n",
    "    target_scaler=target_scaler)\n",
    "\n",
    "    residuals = target_test - prediction\n",
    "    sigma_sq = np.var(residuals)\n",
    "    log_likelihood = -0.5 * np.sum((residuals ** 2) / sigma_sq + np.log(2 * np.pi * sigma_sq))\n",
    "\n",
    "    p = prediction.mean(axis=0)\n",
    "    mape = mean_absolute_percentage_error(target_test, p)\n",
    "    \n",
    "    return extra_features_train, log_likelihood, mape, mmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a7d09-e92d-445d-b02a-d632763888fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the needed data for function_mmm() \n",
    "\n",
    "mdsp_cols = ['media_channel_1', 'media_channel_2', 'media_channel_3', 'media_channel_4', 'media_channel_5']\n",
    "test_data_period_size = 4\n",
    "degrees_seasonality = 2\n",
    "costs_multiply = 0.15\n",
    "\n",
    "# model_name and degrees_seasonality are chosen by mmm_test_choose(df) function\n",
    "model_name = 'hill_adstock' \n",
    "degrees_seasonality = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d09bba-6cc2-4516-9b6e-7bfcae56ac4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extra_features_train_baseline, log_likelihood_baseline, mape_baseline, mmm_baseline = function_mmm(mdsp_cols = mdsp_cols, \n",
    "                            test_data_period_size = test_data_period_size,\n",
    "                            control_vars = ['macroeconomic_factor_1', 'macroeconomic_factor_2'],\n",
    "                            model_name = model_name,\n",
    "                            degrees_seasonality = degrees_seasonality,\n",
    "                            costs_multiply = costs_multiply)\n",
    "\n",
    "extra_features_train_control, log_likelihood_control, mape_control, mmm_control = function_mmm(mdsp_cols = mdsp_cols, \n",
    "                            test_data_period_size = test_data_period_size,\n",
    "                            control_vars = ['macroeconomic_factor_1', 'macroeconomic_factor_2', 'holidays'],\n",
    "                            model_name = model_name,\n",
    "                            degrees_seasonality = degrees_seasonality,\n",
    "                            costs_multiply = costs_multiply)\n",
    "\n",
    "extra_features_train_baseline_inter, log_likelihood_baseline_inter, mape_baseline_inter, mmm_baseline_inter = function_mmm(mdsp_cols = mdsp_cols, \n",
    "                            test_data_period_size = test_data_period_size,\n",
    "                            control_vars = ['macroeconomic_factor_1_inter', 'macroeconomic_factor_2_inter'],\n",
    "                            model_name = model_name,\n",
    "                            degrees_seasonality = degrees_seasonality,\n",
    "                            costs_multiply = costs_multiply)\n",
    "\n",
    "extra_features_train_control_inter, log_likelihood_control_inter, mape_control_inter, mmm_control_inter = function_mmm(mdsp_cols = mdsp_cols, \n",
    "                            test_data_period_size = test_data_period_size,\n",
    "                            control_vars = ['macroeconomic_factor_1_inter', 'macroeconomic_factor_2_inter', 'holidays'],\n",
    "                            model_name = model_name,\n",
    "                            degrees_seasonality = degrees_seasonality,\n",
    "                            costs_multiply = costs_multiply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf36990-b06f-41c9-b070-38e578c5182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results whether control variable (holidays) and interpolation significantly improves the model are shown\n",
    "\n",
    "# Compute LRT statistic\n",
    "lrt_stat = -2 * (log_likelihood_baseline - log_likelihood_control)\n",
    "lrt_stat_inter = -2 * (log_likelihood_baseline_inter - log_likelihood_control_inter)\n",
    "\n",
    "# Degrees of freedom (difference in model parameters)\n",
    "df_diff = extra_features_train_control.shape[1] - extra_features_train_baseline.shape[1]\n",
    "df_diff_inter = extra_features_train_control_inter.shape[1] - extra_features_train_baseline_inter.shape[1]\n",
    "\n",
    "# Compute p-value\n",
    "p_value = 1 - chi2.cdf(lrt_stat, df=df_diff)\n",
    "p_value_inter = 1 - chi2.cdf(lrt_stat, df=df_diff_inter)\n",
    "\n",
    "print('DATA WITHOUT INTERPOLATION')\n",
    "# Print results\n",
    "print(\"Baseline Log-Likelihood:\", log_likelihood_baseline)\n",
    "print(\"Control Model Log-Likelihood:\", log_likelihood_control)\n",
    "print(\"LRT Statistic:\", lrt_stat)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "print('INTERPOLATED DATA')\n",
    "# Print results\n",
    "print(\"Baseline Log-Likelihood_inter:\", log_likelihood_baseline_inter)\n",
    "print(\"Control Model Log-Likelihood_inter:\", log_likelihood_control_inter)\n",
    "print(\"LRT Statistic Interpolation:\", lrt_stat_inter)\n",
    "print(\"p-value interpolation:\", p_value_inter)\n",
    "\n",
    "print('DATA WITHOUT INTERPOLATION')\n",
    "# control variable = holidays\n",
    "if p_value < 0.05:\n",
    "    print(\"✅ The control variable significantly improves the model!\")\n",
    "else:\n",
    "    print(\"❌ The control variable does NOT significantly improve the model.\")\n",
    "\n",
    "print('INTERPOLATED DATA')\n",
    "# control variable = holidays\n",
    "if p_value_inter < 0.05:\n",
    "    print(\"✅ The control variable significantly improves the model!\")\n",
    "else:\n",
    "    print(\"❌ The control variable does NOT significantly improve the model.\")\n",
    "\n",
    "print('DATA WITHOUT INTERPOLATION')\n",
    "print(\"Baseline MAPE:\", mape_baseline)\n",
    "print(\"With Control MAPE:\", mape_control)\n",
    "\n",
    "print('INTERPOLATED DATA')\n",
    "print(\"Baseline MAPE inter:\", mape_baseline_inter)\n",
    "print(\"With Control MAPE inter:\", mape_control_inter)\n",
    "\n",
    "print(\"WITHOUT HOLIDAYS\")\n",
    "if mape_baseline_inter < mape_baseline:\n",
    "    print(\"✅ The interpolation significantly improves the model!\")\n",
    "else:\n",
    "    print(\"❌ The interpolation does NOT significantly improve the model.\")\n",
    "\n",
    "print(\"WITH HOLIDAYS\")\n",
    "if mape_control_inter < mape_control:\n",
    "    print(\"✅ The interpolation significantly improves the model!\")\n",
    "else:\n",
    "    print(\"❌ The interpolation does NOT significantly improve the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a3e209-82b8-478f-b966-8718379e7c9b",
   "metadata": {},
   "source": [
    "# Getting more specific results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d011961d-0ee6-4ccd-9f79-1ae565386754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_warmup=1000\n",
    "number_samples=1000\n",
    "mmm = lightweight_mmm.LightweightMMM(model_name=model_name)\n",
    "mmm.fit( media=media_data_train, media_prior=costs, target=target_train,\n",
    "         extra_features=extra_features_train,\n",
    "        degrees_seasonality=degrees_seasonality,\n",
    "        seasonality_frequency=12,\n",
    "        number_warmup=number_warmup, number_samples=number_samples, media_names = mdsp_cols, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced061d-c86c-45b9-9330-779fa5520a70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Once training is finished, you can check the summary of your trace: \n",
    "The important point here is to check whether r hat values for all parameters are less than 1.1. \n",
    "This is a checkpoint when you run Bayesian modeling.'''\n",
    "mmm.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8ef6b-f034-4ab0-a346-28a926d6f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking R_hat\n",
    "f = io.StringIO()\n",
    "with redirect_stdout(f):\n",
    "    mmm.print_summary()\n",
    "df_mmm = summary_df(f.getvalue())\n",
    "if len(df_mmm.loc[df_mmm.r_hat.astype(float) >= 1.1]) > 0:\n",
    "    r_hat = \"R_HAT >= 1.1\"\n",
    "else:\n",
    "    r_hat = \"R_HAT ok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a3d9f4-af7a-42fd-84b9-f8eb83097ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining a plot of the train dataset\n",
    "plot.plot_model_fit(mmm, target_scaler=target_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d5ed0-12f0-4a3c-a3a3-437d07b3c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining a plot of the media contribution \n",
    "\n",
    "media_contribution, roi_hat = mmm.get_posterior_metrics(target_scaler=target_scaler, cost_scaler=cost_scaler)\n",
    "plot.plot_media_baseline_contribution_area_plot(media_mix_model=mmm,\n",
    "                                                target_scaler=target_scaler,\n",
    "                                                fig_size=(20,10),\n",
    "                                                channel_names = mdsp_cols\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be87e0-0057-4dfb-8c39-08a36c72f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining a plot of the test dataset\n",
    "# We have to scale the test media data if we have not done so before\n",
    "\n",
    "new_predictions = mmm.predict(media=media_scaler.transform(media_data_test),\n",
    "                              extra_features=extra_features_scaler.transform(extra_features_test),\n",
    "                              seed=SEED)\n",
    "print(new_predictions.shape)\n",
    "\n",
    "plot.plot_out_of_sample_model_fit(out_of_sample_predictions=new_predictions,\n",
    "                                 out_of_sample_target=target_scaler.transform(target[split_point:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0bbc6-8dbf-4645-b0f0-cfd498cf64d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate Weighted Average Percentage Error\n",
    "def wape(y_true, y_pred):\n",
    "    error = np.abs(y_true - y_pred)\n",
    "    return np.sum(error) / np.sum(y_true)\n",
    "\n",
    "# WAPER test and WAPE train\n",
    "prediction = mmm.predict(\n",
    "    media=media_scaler.transform(media_data_test),\n",
    "    extra_features=extra_features_scaler.transform(extra_features_test),\n",
    "    target_scaler=target_scaler,\n",
    "    seed=SEED)\n",
    "p = prediction.mean(axis = 0)\n",
    "\n",
    "print(f\"WAPE_test:{wape(target_test, p)*100}\")\n",
    "\n",
    "true_train = target_scaler.inverse_transform(target_train)\n",
    "\n",
    "posterior_pred = mmm.trace[\"mu\"]\n",
    "posterior_pred = target_scaler.inverse_transform(posterior_pred)\n",
    "p = posterior_pred.mean(axis = 0)\n",
    "\n",
    "print(f\"WAPE_train:{wape(true_train, p)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a02f8cc-30d9-41e0-a4ef-06269eb031e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining coefficients for each media channel\n",
    "mdsp_cols = mdsp_cols\n",
    "plot.plot_media_channel_posteriors(media_mix_model=mmm, channel_names = mdsp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1da773-4b75-4cd5-a84b-4f49696e2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting percentages of contribution\n",
    "\n",
    "media_contributionss = jnp.einsum(\"stc, sc->stc\",\n",
    "                              mmm.trace[\"media_transformed\"],\n",
    "                              mmm.trace[\"coef_media\"])\n",
    "\n",
    "mean_contributions = np.mean(media_contributionss, axis=0) # Shape: (51, 6)\n",
    "\n",
    "total_contributions_per_time = np.sum(mean_contributions, axis=1)  # Shape: (51,)\n",
    "\n",
    "# Step 3: Calculate percentage contribution for each channel at each time point\n",
    "percentage_contributions = (mean_contributions.T / total_contributions_per_time).T * 100  # Shape: (51, 6)\n",
    "\n",
    "# Step 4: Create a DataFrame for better visualization\n",
    "df['month'] = df['index1']\n",
    "time_points_range = df['index1']\n",
    "channel_names = mdsp_cols\n",
    "percentage_contributions_df = pd.DataFrame(percentage_contributions, columns=channel_names, index=time_points_range)\n",
    "mean_contributions_df = pd.DataFrame(mean_contributions, columns=channel_names, index=time_points_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1ae23-28f9-48fe-912f-594c98e2b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frame with media contribution in %\n",
    "excel_data = df.merge(percentage_contributions_df.reset_index(), on = 'index1', how = 'outer', suffixes = (f'', f'_contr_%'))\n",
    "excel_data = excel_data.drop(columns = ['index1', 'month'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a0d40-0665-450a-a1a7-682c16ea38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Media Contribution for the last month\n",
    "\n",
    "# Select the single row (as Series)\n",
    "row = percentage_contributions_df.iloc[-1:].iloc[0]\n",
    "\n",
    "# Create a pie chart\n",
    "plt.figure(figsize=(7, 7))\n",
    "row.plot.pie(\n",
    "    autopct=\"%1.1f%%\",  # Display percentage\n",
    "    startangle=80,      # Start at 90 degrees for better orientation\n",
    "    legend=True,        # Add legend\n",
    "    ylabel=\"\",          # Remove default ylabel\n",
    "    title=\"Media Channel Contributions\", fontsize=12\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a0390-33ee-4038-b128-a0c36ab3b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the plot of nedia channel effectiveness\n",
    "# Shows how much sales a dollar in the media channel brings\n",
    "plot.plot_bars_media_metrics(metric=media_contributionss, metric_name=\"Aquisition effectiveness\", channel_names=mdsp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8507285b-c810-447a-8c6f-429303ef5553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curves for each media channel\n",
    "plot.plot_response_curves(media_mix_model=mmm, target_scaler=target_scaler, seed=SEED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
